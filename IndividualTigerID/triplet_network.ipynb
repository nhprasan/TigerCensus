{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a9d6d74",
   "metadata": {},
   "source": [
    "Reference paper: https://arxiv.org/pdf/1412.6622"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c743cc",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "To store the samples and their corresponding labels.\n",
    "\n",
    "#### Metric Learning (learning a distance metric between individuals)\n",
    "\n",
    "- Triplet Network (triplet loss) = triplets of images: an anchor image, a positive image (same individual as the anchor), and a negative image (different individual) are used to recognize the similarity or differences between items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e0906a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71186589",
   "metadata": {},
   "source": [
    "#### Transformation 1: Input image dimensions must be uniform, typically 224x224 for ResNet50 CNN backbone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23fc4b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fc640bf",
   "metadata": {},
   "source": [
    "#### Transformation 2: Preserve the aspect ratio of the original image\n",
    "\n",
    "We can preserve the aspect ratio while resizing images by padding them with zeros, ensuring that the tigers' features remain undistorted during the resizing process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c539ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimental\n",
    "\n",
    "# from torchvision import transforms\n",
    "\n",
    "# aspectratio_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb70854",
   "metadata": {},
   "source": [
    "# ResNet-50 Backbone CNN to generate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba61465",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3811f7b",
   "metadata": {},
   "source": [
    "# DataLoaders\n",
    "To wrap an iterable around the Dataset to enable easy access to the samples. We have loaded the dataset into the DataLoader and each iteration returns a batch of train_features and train_labels.\n",
    "\n",
    "While training a model, we typically want to pass samples in “minibatches”, reshuffle the data at every epoch to reduce model overfitting. DataLoader is an iterable that abstracts this complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6badbde8",
   "metadata": {},
   "source": [
    "# Triplet Network\n",
    "\n",
    "The objective is to develop an embedding in which the distance between similar samples is smaller than the distance between dissimilar ones.\n",
    "\n",
    "- Anchor is a base image\n",
    "- Positive is a sample of the same class as the anchor\n",
    "- Negative is a sample of a different class from the anchor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de833c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# For Triplet Network\n",
    "train_dataset_triplet = TigerReIDDataset(img_dir='reid_data', mode='triplet', train=True, transform=resize_transform)\n",
    "train_loader_triplet = DataLoader(train_dataset_triplet, batch_size=3, shuffle=True) # after we iterate over all batches the data is shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b81d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_triplet_batch(batch):\n",
    "    \"\"\"\n",
    "    Show a batch of triplet images (anchor, positive, negative).\n",
    "    \n",
    "    Args:\n",
    "        batch: A tuple containing (anchor_images, positive_images, negative_images)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Unpack the batch into anchor, positive, and negative images\n",
    "    anchor_images, positive_images, negative_images = batch\n",
    "    batch_size = len(anchor_images)\n",
    "    \n",
    "    # Create a figure with subplots: each row will show an anchor, positive, and negative image\n",
    "    fig, axes = plt.subplots(batch_size, 3, figsize=(15, 5 * batch_size))\n",
    "    fig.suptitle(\"Triplet Network Image Sets\", fontsize=16)\n",
    "    \n",
    "    # Loop through each triplet in the batch\n",
    "    for i in range(batch_size):\n",
    "        for j, img in enumerate([anchor_images[i], positive_images[i], negative_images[i]]):\n",
    "            ax = axes[i, j]\n",
    "            \n",
    "            # Convert from tensor to numpy array and transpose dimensions for display\n",
    "            img_np = img.numpy().transpose((1, 2, 0))  # Change shape from (C, H, W) to (H, W, C)\n",
    "            \n",
    "            # Denormalize the image (undo normalization applied during preprocessing)\n",
    "            mean = np.array([0.485, 0.456, 0.406])\n",
    "            std = np.array([0.229, 0.224, 0.225])\n",
    "            img_np = std * img_np + mean\n",
    "            img_np = np.clip(img_np, 0, 1)\n",
    "            \n",
    "            ax.imshow(img_np)\n",
    "            ax.axis('off')\n",
    "            \n",
    "            # Set titles for each image\n",
    "            if j == 0:\n",
    "                ax.set_title(\"Anchor Image\")\n",
    "            elif j == 1:\n",
    "                ax.set_title(\"Positive Image\")\n",
    "            else:\n",
    "                ax.set_title(\"Negative Image\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Get a batch of data\n",
    "dataiter_triplet = iter(train_loader_triplet)\n",
    "batch_triplet = next(dataiter_triplet)\n",
    "\n",
    "# Show the triplet batch\n",
    "show_triplet_batch(batch_triplet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70f7083",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e470853f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7719b7b7",
   "metadata": {},
   "source": [
    "### TripletLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca7e273",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9d5f690",
   "metadata": {},
   "source": [
    "### Performance\n",
    "\n",
    "The system provides N potential candidates, from which the user manually selects the relevant Tiger individual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bbcbf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
